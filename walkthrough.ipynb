{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1OtjGkmM_288pb5nW1eWbyjPebcSZFnat",
      "authorship_tag": "ABX9TyM7DWxe/1Bvvq6LB55e47Mu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orynycz/llm/blob/main/walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Get access to the model at https://huggingface.co/meta-llama/Llama-2-7b-hf"
      ],
      "metadata": {
        "id": "Q43ulZyJFj6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-jwINkkgMIU"
      },
      "outputs": [],
      "source": [
        "# Step 1: Select A100 as the hardware accelerator\n",
        "\n",
        "# Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
        "# This is a comment and not executable code.  The user must manually change the runtime type in the Colab interface.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Install torchtune\n",
        "# Takes 1 minute 30 seconds\n",
        "# Reference: https://pytorch.org/torchtune/main/install.html\n",
        "\n",
        "# Suppress output, remove to troubleshoot\n",
        "%%capture\n",
        "\n",
        "# Install stable version of pre-requisite PyTorch libraries using pip\n",
        "!pip install torch torchvision torchao\n",
        "\n",
        "!pip install torchtune\n",
        "\n",
        "# Install the EleutherAI evaluation harness\n",
        "! pip install lm_eval==0.4.*"
      ],
      "metadata": {
        "id": "2Ew496vghLx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0xh-CRjB7ro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49d5073d-f953-4bef-d54d-3fb0c842359b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Set storage and workspace\n",
        "# Takes 11 seconds\n",
        "\n",
        "project_name = 'walkthru' # set this!\n",
        "experiment_number = '00' # set this!\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specifiy directories\n",
        "import os\n",
        "os.environ['STORAGE'] = f'/content/drive/MyDrive/{project_name}/{experiment_number}'\n",
        "os.environ['LAB'] = '/content'\n",
        "\n",
        "# Ensure checkpoint and output directories exist\n",
        "!mkdir -p $STORAGE/untuned\n",
        "!mkdir -p $STORAGE/tuned\n",
        "!mkdir -p $LAB/tuned"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Download the Hugging Face format Llama2 7B model\n",
        "# Takes 2 minutes 13 seconds\n",
        "\n",
        "# IMPORTANT: Click the key symbol on the left sidebar to go to the Secrets section.\n",
        "# Ensure there is an entry named \"huggingface\" and that \"Notebook access\" is checked.\n",
        "# Ensure the secret value is correct. If you don't have a key, get one here: https://huggingface.co/meta-llama/Llama-2-7b-hf\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['ACCESS_TOKEN'] = userdata.get('huggingface')\n",
        "\n",
        "!tune download \\\n",
        "meta-llama/Llama-2-7b-hf \\\n",
        "--output-dir $STORAGE/untuned \\\n",
        "--hf-token $ACCESS_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzoS5dT681A2",
        "outputId": "5c71d334-9d91-4c7c-be70-860c7d0dcdb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignoring files matching the following patterns: None\n",
            ".gitattributes: 100% 1.58k/1.58k [00:00<00:00, 11.9MB/s]\n",
            "LICENSE.txt: 100% 7.02k/7.02k [00:00<00:00, 37.0MB/s]\n",
            "README.md: 100% 22.3k/22.3k [00:00<00:00, 87.5MB/s]\n",
            "Responsible-Use-Guide.pdf: 100% 1.25M/1.25M [00:00<00:00, 21.6MB/s]\n",
            "USE_POLICY.md: 100% 4.77k/4.77k [00:00<00:00, 37.9MB/s]\n",
            "config.json: 100% 609/609 [00:00<00:00, 3.40MB/s]\n",
            "generation_config.json: 100% 188/188 [00:00<00:00, 1.68MB/s]\n",
            "model-00001-of-00002.safetensors: 100% 9.98G/9.98G [00:40<00:00, 249MB/s]\n",
            "model-00002-of-00002.safetensors: 100% 3.50G/3.50G [00:14<00:00, 242MB/s]\n",
            "model.safetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 36.7MB/s]\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.98G/9.98G [00:48<00:00, 206MB/s]\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.50G/3.50G [00:16<00:00, 215MB/s]\n",
            "pytorch_model.bin.index.json: 100% 26.8k/26.8k [00:00<00:00, 36.9MB/s]\n",
            "special_tokens_map.json: 100% 414/414 [00:00<00:00, 3.77MB/s]\n",
            "tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 7.31MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 3.28MB/s]\n",
            "tokenizer_config.json: 100% 776/776 [00:00<00:00, 5.27MB/s]\n",
            "Successfully downloaded model repo and wrote to the following locations:\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/.cache\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/.gitattributes\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/LICENSE.txt\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/README.md\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/Responsible-Use-Guide.pdf\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/USE_POLICY.md\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/config.json\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/generation_config.json\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/model-00001-of-00002.safetensors\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/model-00002-of-00002.safetensors\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/model.safetensors.index.json\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/pytorch_model-00001-of-00002.bin\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/pytorch_model-00002-of-00002.bin\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/pytorch_model.bin.index.json\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/special_tokens_map.json\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/tokenizer.json\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/tokenizer.model\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/tokenizer_config.json\n",
            "/content/drive/MyDrive/walkthru/00/checkpoint/original_repo_id.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Copy checkpoint folder from storage to working directory\n",
        "# Takes 4 minutes 20 seconds\n",
        "\n",
        "!cp -r $STORAGE/untuned $LAB/untuned\n"
      ],
      "metadata": {
        "id": "Gx7-Mhlu3CaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Generate an answer to a question using the untuned model\n",
        "\n",
        "!mkdir -p $LAB/generated\n",
        "\n",
        "# Takes 1 minute 19 seconds on T4 GPU hardware accelerator\n",
        "!tune run generate --config generation \\\n",
        "  output_dir=$LAB/generated \\\n",
        "  checkpoint_dir=$LAB/untuned \\\n",
        "  checkpointer.checkpoint_dir=$LAB/untuned \\\n",
        "  checkpointer.checkpoint_files=[model-00001-of-00002.safetensors,model-00002-of-00002.safetensors] \\\n",
        "  tokenizer.path=$LAB/untuned/tokenizer.model \\\n",
        "  temperature=0.6 \\\n",
        "  prompt.user=\"Is it true that two plus two equals five?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu1Owo7C7Rfa",
        "outputId": "7b70b34e-14f4-4378-df1c-b061335f11f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils._logging:Running InferenceRecipe with resolved config:\n",
            "\n",
            "checkpoint_dir: /content/untuned\n",
            "checkpointer:\n",
            "  _component_: torchtune.training.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /content/untuned\n",
            "  checkpoint_files:\n",
            "  - model-00001-of-00002.safetensors\n",
            "  - model-00002-of-00002.safetensors\n",
            "  model_type: LLAMA2\n",
            "  output_dir: /content/generated\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_kv_cache: true\n",
            "max_new_tokens: 300\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "output_dir: /content/generated\n",
            "prompt:\n",
            "  system: null\n",
            "  user: Is it true that two plus two equals five?\n",
            "quantizer: null\n",
            "seed: 1234\n",
            "temperature: 0.6\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  max_seq_len: null\n",
            "  path: /content/untuned/tokenizer.model\n",
            "  prompt_template: null\n",
            "top_k: 300\n",
            "\n",
            "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
            "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils._logging:Is it true that two plus two equals five?\n",
            "I don't know. I'm not a mathematician.\n",
            "But the idea that two plus two equals five is a common and accepted one. I see it all the time.\n",
            "For instance, a lot of people have the idea that if you take two people who are good at one thing and put them together then they will be good at two things.\n",
            "In fact, this is not always true.\n",
            "It is not true if the two people are not compatible, or if they don't work well together, or if they don't have the same goals.\n",
            "It is not true if one person is not good at the thing that the other person is good at.\n",
            "It is not true if the two people don't trust each other, or if they are not willing to support each other.\n",
            "It is not true if the two people are not willing to learn from each other.\n",
            "It is not true if the two people are not willing to make compromises.\n",
            "It is not true if the two people are not willing to put in the work.\n",
            "It is not true if the two people are not willing to be flexible.\n",
            "It is not true if the two people are not willing to be patient.\n",
            "It is not true if the two people are not willing to communicate.\n",
            "It is not true if the two people are not willing to be honest.\n",
            "It is not true if the two people are not willing to be open\n",
            "INFO:torchtune.utils._logging:Time for inference: 23.15 sec total, 12.96 tokens/sec\n",
            "INFO:torchtune.utils._logging:Bandwidth achieved: 177.62 GB/s\n",
            "INFO:torchtune.utils._logging:Memory used: 13.95 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step x - Evaluate untuned model\n",
        "# Takes 2 minutes 50 seconds\n",
        "\n",
        "!tune run eleuther_eval \\\n",
        "--config eleuther_evaluation \\\n",
        "checkpointer.checkpoint_dir=$LAB/untuned \\\n",
        "tokenizer.path=$LAB/untuned/tokenizer.model \\\n",
        "batch_size=4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZvLms4ZZoLh",
        "outputId": "cccfeece-91e6-4d4d-b5ce-137678ddf0a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-19 09:32:11.709802: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-02-19 09:32:11.728239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739957531.750140   34990 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739957531.756824   34990 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-19 09:32:11.778818: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:torchtune.utils._logging:Running EleutherEvalRecipe with resolved config:\n",
            "\n",
            "batch_size: 4\n",
            "checkpointer:\n",
            "  _component_: torchtune.training.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /content/untuned\n",
            "  checkpoint_files:\n",
            "  - pytorch_model-00001-of-00002.bin\n",
            "  - pytorch_model-00002-of-00002.bin\n",
            "  model_type: LLAMA2\n",
            "  output_dir: ./\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_kv_cache: true\n",
            "limit: null\n",
            "max_seq_length: 4096\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "output_dir: ./\n",
            "quantizer: null\n",
            "seed: 1234\n",
            "tasks:\n",
            "- truthfulqa_mc2\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  max_seq_len: null\n",
            "  path: /content/untuned/tokenizer.model\n",
            "\n",
            "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils._logging:Running evaluation on the following tasks: ['truthfulqa_mc2']\n",
            "INFO:lm-eval:Building contexts for truthfulqa_mc2 on rank 0...\n",
            "100% 817/817 [00:01<00:00, 770.32it/s]\n",
            "INFO:lm-eval:Running loglikelihood requests\n",
            "Running loglikelihood requests: 100% 5882/5882 [02:04<00:00, 47.31it/s]\n",
            "INFO:torchtune.utils._logging:Eval completed in 130.20 seconds.\n",
            "INFO:torchtune.utils._logging:Max memory allocated: 27.64 GB\n",
            "INFO:torchtune.utils._logging:\n",
            "\n",
            "|    Tasks     |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
            "|--------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
            "|truthfulqa_mc2|      2|none  |     0|acc   |↑  |0.3892|±  |0.0136|\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Perform fine tuning\n",
        "# Takes 2-3 hours\n",
        "\n",
        "!tune run lora_finetune_single_device \\\n",
        "--config llama2/7B_lora_single_device \\\n",
        "checkpointer.checkpoint_dir=$LAB/untuned \\\n",
        "tokenizer.path=$LAB/checkpoint/tokenizer.model \\\n",
        "checkpointer.output_dir=$LAB/tuned\n",
        "\n",
        "# Copy recipe to local storage\n",
        "!tune cp llama2/7B_lora_single_device $STORAGE/recipe.yaml\n",
        "\n",
        "# Put output into Storage\n",
        "!cp -r $LAB/tuned $STORAGE\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMTFos6qOl4w",
        "outputId": "102efda9-7f1b-425d-b20d-a4d8ae5cd907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils._logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
            "\n",
            "batch_size: 2\n",
            "checkpointer:\n",
            "  _component_: torchtune.training.FullModelHFCheckpointer\n",
            "  adapter_checkpoint: null\n",
            "  checkpoint_dir: /content/checkpoint\n",
            "  checkpoint_files:\n",
            "  - pytorch_model-00001-of-00002.bin\n",
            "  - pytorch_model-00002-of-00002.bin\n",
            "  model_type: LLAMA2\n",
            "  output_dir: /content/output\n",
            "  recipe_checkpoint: null\n",
            "compile: false\n",
            "dataset:\n",
            "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
            "  packed: false\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_activation_checkpointing: true\n",
            "enable_activation_offloading: false\n",
            "epochs: 1\n",
            "gradient_accumulation_steps: 8\n",
            "log_every_n_steps: 1\n",
            "log_peak_memory_stats: true\n",
            "loss:\n",
            "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
            "lr_scheduler:\n",
            "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
            "  num_warmup_steps: 100\n",
            "max_steps_per_epoch: null\n",
            "metric_logger:\n",
            "  _component_: torchtune.training.metric_logging.DiskLogger\n",
            "  log_dir: /tmp/torchtune/llama2_7B/lora_single_device/logs\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.lora_llama2_7b\n",
            "  apply_lora_to_mlp: true\n",
            "  apply_lora_to_output: false\n",
            "  lora_alpha: 16\n",
            "  lora_attn_modules:\n",
            "  - q_proj\n",
            "  - v_proj\n",
            "  - output_proj\n",
            "  lora_dropout: 0.0\n",
            "  lora_rank: 8\n",
            "optimizer:\n",
            "  _component_: torch.optim.AdamW\n",
            "  fused: true\n",
            "  lr: 0.0003\n",
            "  weight_decay: 0.01\n",
            "output_dir: /tmp/torchtune/llama2_7B/lora_single_device\n",
            "profiler:\n",
            "  _component_: torchtune.training.setup_torch_profiler\n",
            "  active_steps: 2\n",
            "  cpu: true\n",
            "  cuda: true\n",
            "  enabled: false\n",
            "  num_cycles: 1\n",
            "  output_dir: /tmp/torchtune/llama2_7B/lora_single_device/profiling_outputs\n",
            "  profile_memory: false\n",
            "  record_shapes: true\n",
            "  wait_steps: 5\n",
            "  warmup_steps: 5\n",
            "  with_flops: false\n",
            "  with_stack: false\n",
            "resume_from_checkpoint: false\n",
            "save_adapter_weights_only: false\n",
            "seed: null\n",
            "shuffle: true\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  max_seq_len: null\n",
            "  path: /content/checkpoint/tokenizer.model\n",
            "\n",
            "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 4002267682. Local seed is seed + rank = 4002267682 + 0\n",
            "INFO:torchtune.utils._logging:Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
            "Writing logs to /tmp/torchtune/llama2_7B/lora_single_device/logs/log_1739868453.txt\n",
            "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils._logging:Memory stats after model init:\n",
            "\tGPU peak memory allocation: 13.03 GiB\n",
            "\tGPU peak memory reserved: 13.04 GiB\n",
            "\tGPU peak memory active: 13.03 GiB\n",
            "INFO:torchtune.utils._logging:Tokenizer is initialized from file.\n",
            "INFO:torchtune.utils._logging:Optimizer and loss are initialized.\n",
            "INFO:torchtune.utils._logging:Loss is initialized.\n",
            "INFO:torchtune.utils._logging:Dataset and Sampler are initialized.\n",
            "INFO:torchtune.utils._logging:Learning rate scheduler is initialized.\n",
            "WARNING:torchtune.utils._logging: Profiling disabled.\n",
            "INFO:torchtune.utils._logging: Profiler config after instantiation: {'enabled': False}\n",
            "1|3235|Loss: 0.6866737604141235: 100% 3235/3235 [2:45:22<00:00,  3.04s/it]INFO:torchtune.utils._logging:Starting checkpoint save...\n",
            "INFO:torchtune.utils._logging:Model checkpoint of size 9.29 GiB saved to /content/output/epoch_0/ft-model-00001-of-00002.safetensors\n",
            "INFO:torchtune.utils._logging:Model checkpoint of size 3.26 GiB saved to /content/output/epoch_0/ft-model-00002-of-00002.safetensors\n",
            "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.03 GiB saved to /content/output/epoch_0/adapter_model.pt\n",
            "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.03 GiB saved to /content/output/epoch_0/adapter_model.safetensors\n",
            "INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to /content/output/epoch_0/adapter_config.json\n",
            "INFO:torchtune.utils._logging:Saving final epoch checkpoint.\n",
            "INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
            "INFO:torchtune.utils._logging:Checkpoint saved in 63.55 seconds.\n",
            "1|3235|Loss: 0.6866737604141235: 100% 3235/3235 [2:46:26<00:00,  3.09s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step x - Copy fine-tuned model from storage to lab\n",
        "# Takes 4 minutes 33 seconds\n",
        "\n",
        "!cp -r $STORAGE/tuned $LAB"
      ],
      "metadata": {
        "id": "Ub94Nb9QAea9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step x - Evaluate fine-tuned model\n",
        "# Takes 2 minutes 50 seconds\n",
        "\n",
        "!tune run eleuther_eval \\\n",
        "--config eleuther_evaluation \\\n",
        "checkpointer.checkpoint_dir=$LAB/tuned/epoch_0 \\\n",
        "checkpointer.checkpoint_files=[ft-model-00001-of-00002.safetensors,ft-model-00002-of-00002.safetensors] \\\n",
        "tokenizer.path=$LAB/tuned/epoch_0/tokenizer.model \\\n",
        "batch_size=4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnZ913wtikXr",
        "outputId": "bcf1f366-60ef-4fad-96de-a44547d35791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n",
            "2025-02-19 09:52:16.965744: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-02-19 09:52:16.982921: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739958737.004422   40614 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739958737.010992   40614 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-19 09:52:17.032586: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "INFO:torchtune.utils._logging:Running EleutherEvalRecipe with resolved config:\n",
            "\n",
            "batch_size: 4\n",
            "checkpointer:\n",
            "  _component_: torchtune.training.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /content/tuned/epoch_0\n",
            "  checkpoint_files:\n",
            "  - ft-model-00001-of-00002.safetensors\n",
            "  - ft-model-00002-of-00002.safetensors\n",
            "  model_type: LLAMA2\n",
            "  output_dir: ./\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_kv_cache: true\n",
            "limit: null\n",
            "max_seq_length: 4096\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "output_dir: ./\n",
            "quantizer: null\n",
            "seed: 1234\n",
            "tasks:\n",
            "- truthfulqa_mc2\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  max_seq_len: null\n",
            "  path: /content/tuned/epoch_0/tokenizer.model\n",
            "\n",
            "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils._logging:Running evaluation on the following tasks: ['truthfulqa_mc2']\n",
            "INFO:lm-eval:Building contexts for truthfulqa_mc2 on rank 0...\n",
            "100% 817/817 [00:01<00:00, 777.79it/s]\n",
            "INFO:lm-eval:Running loglikelihood requests\n",
            "Running loglikelihood requests: 100% 5882/5882 [02:04<00:00, 47.35it/s]\n",
            "INFO:torchtune.utils._logging:Eval completed in 130.05 seconds.\n",
            "INFO:torchtune.utils._logging:Max memory allocated: 27.64 GB\n",
            "INFO:torchtune.utils._logging:\n",
            "\n",
            "|    Tasks     |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
            "|--------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
            "|truthfulqa_mc2|      2|none  |     0|acc   |↑  |0.4719|±  |0.0145|\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p $LAB/generated\n",
        "\n",
        "# Takes 45 seconds on T4 GPU hardware accelerator\n",
        "!tune run generate --config generation \\\n",
        "  output_dir=$LAB/generated \\\n",
        "  checkpoint_dir=$LAB/tuned/epoch_0 \\\n",
        "  checkpointer.checkpoint_dir=$LAB/tuned/epoch_0 \\\n",
        "  checkpointer.checkpoint_files=[ft-model-00001-of-00002.safetensors,ft-model-00002-of-00002.safetensors] \\\n",
        "  tokenizer.path=$LAB/tuned/epoch_0/tokenizer.model \\\n",
        "  temperature=0.6 \\\n",
        "  prompt.user=\"Is it true that two plus two equals five?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4hl-TeY8h3B",
        "outputId": "22e4e0f9-f84d-4bd6-c740-4ef8bc1d538f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils._logging:Running InferenceRecipe with resolved config:\n",
            "\n",
            "checkpoint_dir: /content/tuned/epoch_0\n",
            "checkpointer:\n",
            "  _component_: torchtune.training.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /content/tuned/epoch_0\n",
            "  checkpoint_files:\n",
            "  - ft-model-00001-of-00002.safetensors\n",
            "  - ft-model-00002-of-00002.safetensors\n",
            "  model_type: LLAMA2\n",
            "  output_dir: /content/generated\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_kv_cache: true\n",
            "max_new_tokens: 300\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "output_dir: /content/generated\n",
            "prompt:\n",
            "  system: null\n",
            "  user: Is it true that two plus two equals five?\n",
            "quantizer: null\n",
            "seed: 1234\n",
            "temperature: 0.6\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  max_seq_len: null\n",
            "  path: /content/tuned/epoch_0/tokenizer.model\n",
            "  prompt_template: null\n",
            "top_k: 300\n",
            "\n",
            "DEBUG:torchtune.utils._logging:Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
            "INFO:torchtune.utils._logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils._logging:Is it true that two plus two equals five?\n",
            "No, it is not true that two plus two equals five. The correct answer is four.\n",
            "Q: Is it true that two plus two equals five?\n",
            "Is 2 plus 2 equals 5?\n",
            "No, 2 plus 2 does not equal 5. The correct answer is 4.\n",
            "What is 2 plus 2 equals?\n",
            "2+2=4. It is true that 2 plus 2 equals 4.\n",
            "Does 2 plus 2 equal 5?\n",
            "No, 2 plus 2 equals 4.\n",
            "Is it true that two plus two equals five?\n",
            "No, it is not true that two plus two equals five. The correct answer is four.\n",
            "Is it true that 2 plus 2 equals 5?\n",
            "No, 2 plus 2 equals 4.\n",
            "How true is the statement 2 plus 2 equals 5?\n",
            "2+2=4, which is obviously not true.\n",
            "What is 2 plus 2 equals?\n",
            "2+2=4.\n",
            "Is the equation 2 plus 2 equals 5?\n",
            "No, that is not true. 2 + 2 is equal to 4.\n",
            "What is 2 plus 2 equals to?\n",
            "What is 2 plus 2 equals?\n",
            "2+2=4, which is obviously true.\n",
            "Is 2 plus 2 equals 5?\n",
            "No, \n",
            "INFO:torchtune.utils._logging:Time for inference: 23.16 sec total, 12.95 tokens/sec\n",
            "INFO:torchtune.utils._logging:Bandwidth achieved: 177.54 GB/s\n",
            "INFO:torchtune.utils._logging:Memory used: 13.95 GB\n"
          ]
        }
      ]
    }
  ]
}